{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5655844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#50. 単語ベクトルの読み込みと表示\n",
    "\"\"\"Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル（300万単語・フレーズ、300次元）をダウンロードし、\n",
    "”United States”の単語ベクトルを表示せよ。ただし、”United States”は内部的には”United_States”と表現されていることに注意せよ。\n",
    "\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "word2vector = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ded4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.61328125e-02, -4.83398438e-02,  2.35351562e-01,  1.74804688e-01,\n",
       "       -1.46484375e-01, -7.42187500e-02, -1.01562500e-01, -7.71484375e-02,\n",
       "        1.09375000e-01, -5.71289062e-02, -1.48437500e-01, -6.00585938e-02,\n",
       "        1.74804688e-01, -7.71484375e-02,  2.58789062e-02, -7.66601562e-02,\n",
       "       -3.80859375e-02,  1.35742188e-01,  3.75976562e-02, -4.19921875e-02,\n",
       "       -3.56445312e-02,  5.34667969e-02,  3.68118286e-04, -1.66992188e-01,\n",
       "       -1.17187500e-01,  1.41601562e-01, -1.69921875e-01, -6.49414062e-02,\n",
       "       -1.66992188e-01,  1.00585938e-01,  1.15722656e-01, -2.18750000e-01,\n",
       "       -9.86328125e-02, -2.56347656e-02,  1.23046875e-01, -3.54003906e-02,\n",
       "       -1.58203125e-01, -1.60156250e-01,  2.94189453e-02,  8.15429688e-02,\n",
       "        6.88476562e-02,  1.87500000e-01,  6.49414062e-02,  1.15234375e-01,\n",
       "       -2.27050781e-02,  3.32031250e-01, -3.27148438e-02,  1.77734375e-01,\n",
       "       -2.08007812e-01,  4.54101562e-02, -1.23901367e-02,  1.19628906e-01,\n",
       "        7.44628906e-03, -9.03320312e-03,  1.14257812e-01,  1.69921875e-01,\n",
       "       -2.38281250e-01, -2.79541016e-02, -1.21093750e-01,  2.47802734e-02,\n",
       "        7.71484375e-02, -2.81982422e-02, -4.71191406e-02,  1.78222656e-02,\n",
       "       -1.23046875e-01, -5.32226562e-02,  2.68554688e-02, -3.11279297e-02,\n",
       "       -5.59082031e-02, -5.00488281e-02, -3.73535156e-02,  1.25976562e-01,\n",
       "        5.61523438e-02,  1.51367188e-01,  4.29687500e-02, -2.08007812e-01,\n",
       "       -4.78515625e-02,  2.78320312e-02,  1.81640625e-01,  2.20703125e-01,\n",
       "       -3.61328125e-02, -8.39843750e-02, -3.69548798e-05, -9.52148438e-02,\n",
       "       -1.25000000e-01, -1.95312500e-01, -1.50390625e-01, -4.15039062e-02,\n",
       "        1.31835938e-01,  1.17675781e-01,  1.91650391e-02,  5.51757812e-02,\n",
       "       -9.42382812e-02, -1.08886719e-01,  7.32421875e-02, -1.15234375e-01,\n",
       "        8.93554688e-02, -1.40625000e-01,  1.45507812e-01,  4.49218750e-02,\n",
       "       -1.10473633e-02, -1.62353516e-02,  4.05883789e-03,  3.75976562e-02,\n",
       "       -6.98242188e-02, -5.46875000e-02,  2.17285156e-02, -9.47265625e-02,\n",
       "        4.24804688e-02,  1.81884766e-02, -1.73339844e-02,  4.63867188e-02,\n",
       "       -1.42578125e-01,  1.99218750e-01,  1.10839844e-01,  2.58789062e-02,\n",
       "       -7.08007812e-02, -5.54199219e-02,  3.45703125e-01,  1.61132812e-01,\n",
       "       -2.44140625e-01, -2.59765625e-01, -9.71679688e-02,  8.00781250e-02,\n",
       "       -8.78906250e-02, -7.22656250e-02,  1.42578125e-01, -8.54492188e-02,\n",
       "       -3.18359375e-01,  8.30078125e-02,  6.34765625e-02,  1.64062500e-01,\n",
       "       -1.92382812e-01, -1.17675781e-01, -5.41992188e-02, -1.56250000e-01,\n",
       "       -1.21582031e-01, -4.95605469e-02,  1.20117188e-01, -3.83300781e-02,\n",
       "        5.51757812e-02, -8.97216797e-03,  4.32128906e-02,  6.93359375e-02,\n",
       "        8.93554688e-02,  2.53906250e-01,  1.65039062e-01,  1.64062500e-01,\n",
       "       -1.41601562e-01,  4.58984375e-02,  1.97265625e-01, -8.98437500e-02,\n",
       "        3.90625000e-02, -1.51367188e-01, -8.60595703e-03, -1.17675781e-01,\n",
       "       -1.97265625e-01, -1.12792969e-01,  1.29882812e-01,  1.96289062e-01,\n",
       "        1.56402588e-03,  3.93066406e-02,  2.17773438e-01, -1.43554688e-01,\n",
       "        6.03027344e-02, -1.35742188e-01,  1.16210938e-01, -1.59912109e-02,\n",
       "        2.79296875e-01,  1.46484375e-01, -1.19628906e-01,  1.76757812e-01,\n",
       "        1.28906250e-01, -1.49414062e-01,  6.93359375e-02, -1.72851562e-01,\n",
       "        9.22851562e-02,  1.33056641e-02, -2.00195312e-01, -9.76562500e-02,\n",
       "       -1.65039062e-01, -2.46093750e-01, -2.35595703e-02, -2.11914062e-01,\n",
       "        1.84570312e-01, -1.85546875e-02,  2.16796875e-01,  5.05371094e-02,\n",
       "        2.02636719e-02,  4.25781250e-01,  1.28906250e-01, -2.77099609e-02,\n",
       "        1.29882812e-01, -1.15722656e-01, -2.05078125e-02,  1.49414062e-01,\n",
       "        7.81250000e-03, -2.05078125e-01, -8.05664062e-02, -2.67578125e-01,\n",
       "       -2.29492188e-02, -8.20312500e-02,  8.64257812e-02,  7.61718750e-02,\n",
       "       -3.66210938e-02,  5.22460938e-02, -1.22070312e-01, -1.44042969e-02,\n",
       "       -2.69531250e-01,  8.44726562e-02, -2.52685547e-02, -2.96630859e-02,\n",
       "       -1.68945312e-01,  1.93359375e-01, -1.08398438e-01,  1.94091797e-02,\n",
       "       -1.80664062e-01,  1.93359375e-01, -7.08007812e-02,  5.85937500e-02,\n",
       "       -1.01562500e-01, -1.31835938e-01,  7.51953125e-02, -7.66601562e-02,\n",
       "        3.37219238e-03, -8.59375000e-02,  1.25000000e-01,  2.92968750e-02,\n",
       "        1.70898438e-01, -9.37500000e-02, -1.09375000e-01, -2.50244141e-02,\n",
       "        2.11914062e-01, -4.44335938e-02,  6.12792969e-02,  2.62451172e-02,\n",
       "       -1.77734375e-01,  1.23046875e-01, -7.42187500e-02, -1.67968750e-01,\n",
       "       -1.08886719e-01, -9.04083252e-04, -7.37304688e-02,  5.49316406e-02,\n",
       "        6.03027344e-02,  8.39843750e-02,  9.17968750e-02, -1.32812500e-01,\n",
       "        1.22070312e-01, -8.78906250e-03,  1.19140625e-01, -1.94335938e-01,\n",
       "       -6.64062500e-02, -2.07031250e-01,  7.37304688e-02,  8.93554688e-02,\n",
       "        1.81884766e-02, -1.20605469e-01, -2.61230469e-02,  2.67333984e-02,\n",
       "        7.76367188e-02, -8.30078125e-02,  6.78710938e-02, -3.54003906e-02,\n",
       "        3.10546875e-01, -2.42919922e-02, -1.41601562e-01, -2.08007812e-01,\n",
       "       -4.57763672e-03, -6.54296875e-02, -4.95605469e-02,  2.22656250e-01,\n",
       "        1.53320312e-01, -1.38671875e-01, -5.24902344e-02,  4.24804688e-02,\n",
       "       -2.38281250e-01,  1.56250000e-01,  5.83648682e-04, -1.20605469e-01,\n",
       "       -9.22851562e-02, -4.44335938e-02,  3.61328125e-02, -1.86767578e-02,\n",
       "       -8.25195312e-02, -8.25195312e-02, -4.05273438e-02,  1.19018555e-02,\n",
       "        1.69921875e-01, -2.80761719e-02,  3.03649902e-03,  9.32617188e-02,\n",
       "       -8.49609375e-02,  1.57470703e-02,  7.03125000e-02,  1.62353516e-02,\n",
       "       -2.27050781e-02,  3.51562500e-02,  2.47070312e-01, -2.67333984e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#50. 単語ベクトルの読み込みと表示\n",
    "\"\"\"Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル（300万単語・フレーズ、300次元）をダウンロードし、\n",
    "”United States”の単語ベクトルを表示せよ。ただし、”United States”は内部的には”United_States”と表現されていることに注意せよ。\n",
    "\"\"\"\n",
    "word2vector[\"United_States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5583528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73107743"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#51. 単語の類似度\n",
    "\"“United States”と”U.S.”のコサイン類似度を計算せよ。\"\n",
    "word2vector.similarity(\"United_States\", \"U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f79dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Unites_States', 0.7877248525619507),\n",
       " ('Untied_States', 0.7541370987892151),\n",
       " ('United_Sates', 0.7400724291801453),\n",
       " ('U.S.', 0.7310774326324463),\n",
       " ('theUnited_States', 0.6404393911361694),\n",
       " ('America', 0.6178410053253174),\n",
       " ('UnitedStates', 0.6167312264442444),\n",
       " ('Europe', 0.6132988929748535),\n",
       " ('countries', 0.6044804453849792),\n",
       " ('Canada', 0.601906955242157)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#52. 類似度の高い単語10件\n",
    "#“United States”とコサイン類似度が高い10語と、その類似度を出力せよ。\n",
    "word2vector.most_similar(\"United_States\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd21ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spains', 0.6056790351867676),\n",
       " ('Barcelona', 0.6044401526451111),\n",
       " ('Spaniards', 0.5837482213973999),\n",
       " ('Málaga', 0.5805597901344299),\n",
       " ('Malaga', 0.5797935724258423),\n",
       " ('Spanish', 0.5793159604072571),\n",
       " ('Catalan', 0.5683084726333618),\n",
       " ('San_Sebastián', 0.5657953023910522),\n",
       " ('Salave_Gold_Deposit', 0.5624399185180664),\n",
       " ('Inveravante_Inversiones_SL', 0.560633659362793)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#53. 加法構成性によるアナロジー\n",
    "\"“Spain”の単語ベクトルから”Madrid”のベクトルを引き、”Athens”のベクトルを足したベクトルを計算し、そのベクトルと類似度の高い10語とその類似度を出力せよ。\"\n",
    "word2vector.most_similar(positive = [\"Spain\", \"Madrid\"],\n",
    "                         negative = [\"Athens\"],\n",
    "                         topn = 10\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "236d10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:05<00:00,  5.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>most_similar</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Baghdad</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>Iraqi</td>\n",
       "      <td>0.635187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>0.713767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>0.723578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0.673462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>0.491975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Baku</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>Dakar_Rally</td>\n",
       "      <td>0.681278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Baku</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>Syria</td>\n",
       "      <td>Syria</td>\n",
       "      <td>0.819194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Baku</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>0.819592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Baku</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>0.640313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Baku</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>0.787626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1         2            3 most_similar  similarity\n",
       "0     Athens      Greece   Baghdad         Iraq        Iraqi    0.635187\n",
       "1     Athens      Greece   Bangkok     Thailand     Thailand    0.713767\n",
       "2     Athens      Greece   Beijing        China        China    0.723578\n",
       "3     Athens      Greece    Berlin      Germany      Germany    0.673462\n",
       "4     Athens      Greece      Bern  Switzerland  Switzerland    0.491975\n",
       "...      ...         ...       ...          ...          ...         ...\n",
       "996     Baku  Azerbaijan     Dakar      Senegal  Dakar_Rally    0.681278\n",
       "997     Baku  Azerbaijan  Damascus        Syria        Syria    0.819194\n",
       "998     Baku  Azerbaijan     Dhaka   Bangladesh   Bangladesh    0.819592\n",
       "999     Baku  Azerbaijan      Doha        Qatar        Qatar    0.640313\n",
       "1000    Baku  Azerbaijan    Dublin      Ireland      Ireland    0.787626\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#54. アナロジーデータでの実験\n",
    "\"\"\"単語アナロジーの評価データをダウンロードし、国と首都に関する事例（: capital-common-countriesセクション）に対して\n",
    "vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し、そのベクトルと類似度が最も高い単語と、その類似度を求めよ。求めた単語と類似度は、各事例と一緒に記録せよ。\"\"\"\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "with open(\"questions-words.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "data = []\n",
    "for idx, line in enumerate(lines):\n",
    "    if idx > 0 and line :\n",
    "        line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "        data.append(line)\n",
    "data = pd.DataFrame(data)\n",
    "data = data.dropna()\n",
    "all_words = word2vector.index_to_key  \n",
    "vectors = word2vector.vectors         \n",
    "result = []\n",
    "similarity = []\n",
    "for i in tqdm(range(len(data[:1000]))): \n",
    "    w1 = data.iloc[i, 0]\n",
    "    w2 = data.iloc[i, 1]\n",
    "    w3 = data.iloc[i, 2]\n",
    "    most_similar_word = word2vector.most_similar(\n",
    "        positive = [w2, w3],\n",
    "        negative = [w1],\n",
    "        topn = 1\n",
    "    )\n",
    "    result.append(most_similar_word[0][0])\n",
    "    similarity.append(most_similar_word[0][1])\n",
    "sample =  data[:1000].assign(most_similar = result, similarity = similarity)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a494ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率は0.788\n"
     ]
    }
   ],
   "source": [
    "#55. アナロジータスクでの正解率\n",
    "\"54の実行結果を用い、意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ。\"\n",
    "correct = 0\n",
    "for i in range(len(sample)):\n",
    "    gold = sample.iloc[i, 3]\n",
    "    pred = sample.iloc[i, 4]\n",
    "    if gold == pred:\n",
    "        correct += 1\n",
    "accuracy = correct / len(sample)\n",
    "print(f\"正解率は{accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bd92cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353/353 [00:00<00:00, 18561.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スピアマン相関係数は0.7000166486272194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#56. WordSimilarity-353での評価\n",
    "\"The WordSimilarity-353 Test Collectionの評価データをダウンロードし、単語ベクトルにより計算される類似度のランキングと、人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ。\"\n",
    "from scipy.stats import spearmanr\n",
    "dev_data = pd.read_csv(\"combined.csv\")\n",
    "human_scores = []\n",
    "model_scores = []\n",
    "for i in tqdm(range(len(dev_data))):\n",
    "    w1, w2, human = dev_data.iloc[i, 0], dev_data.iloc[i, 1], dev_data.iloc[i, 2]\n",
    "    similarity = word2vector.similarity(w1, w2)\n",
    "    human_scores.append(human)\n",
    "    model_scores.append(similarity)\n",
    "\n",
    "corr = spearmanr(human_scores, model_scores)\n",
    "print(f\"スピアマン相関係数は{corr.statistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#57. k-meansクラスタリング\n",
    "#国名に関する単語ベクトルを抽出し、k-meansクラスタリングをクラスタ数k=5として実行せよ。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
